---
layout: post
title:  "Setup NXP LS1043A as a Kubernetes Node in Flannel networking"
categories: jekyll update
---

This post tell how to setup NXP LS1043A as a Kubernetes node in flannel networking, and how to fix some issues during the process.

# Overview

This process need two PCs or VMs, one as Kubernetes master and one as node. Also, one NXP LS1043 box is needed to act as another kubernetes node.

![overview1](/images/nxp1043ask8snode-model.png)

This setup at last deploys a kubernetes service, with two kubernetes pods, each has one container.

![architecture](/images/nxp1043ask8snode-architecture.png)

Flannel is a simple and easy way to configure a layer 3 network fabric designed for Kubernetes.
Here flannel is employed to provide connection between pods.

# Play with PCs
[This reference](https://linuxconfig.org/how-to-install-kubernetes-on-ubuntu-18-04-bionic-beaver-linux) is a guide to setup kubernetes on standard PCs.   
Before continue, please setup a master and a node on two standard PCs according to this reference.

# Setup NXP 1043A
  todo

# Join together

  * on master

```bash
sudo kubeadm init --pod-network-cidr=10.217.0.0/16 -v8
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

kubectl get pods -o wide --all-namespaces
#wait till ready of all pods

kubectl create -f kube-flannel.yml

kubectl get pods -o wide --all-namespaces
#wait till ready of all pods

kubectl apply -f service1.yaml
```

[service1.yaml](/inner/service1.html)


  * on node2 the PC node

kubeadm join 192.168.22.52:6443 --token krhd8f.bsv1h9761uh0lhqf \
    --discovery-token-ca-cert-hash sha256:7992b9fa9788e0b54c671cab9839c4596bd575b616dd2d23010f85485a21d2f9

  * on node1 the NXP 1043A
kubeadm join 192.168.22.52:6443 --token krhd8f.bsv1h9761uh0lhqf \
    --discovery-token-ca-cert-hash sha256:7992b9fa9788e0b54c671cab9839c4596bd575b616dd2d23010f85485a21d2f9

# Issues on NXP LS1043A
  1. dial tcp 10.96.0.1:443: i/o timeout
  Check log of pod `kube-system/kube-flannel-ds-arm64-jg5g6`, 
      * 
I0819 02:38:10.208759      22 main.go:514] Determining IP address of default interface
I0819 02:38:10.213218      22 main.go:527] Using interface with name fm1-mac3 and address 192.168.22.60
I0819 02:38:10.213314      22 main.go:544] Defaulting external address to interface address (192.168.22.60)
E0819 02:38:40.223479      22 main.go:241] Failed to create SubnetManager: error retrieving pod spec for 'kube-system/kube-flannel-ds-arm64-jg5g6': Get https://10.96.0.1:443/api/v1/namespaces/kube-system/pods/kube-flannel-ds-arm64-jg5g6: dial tcp 10.96.0.1:443: i/o timeout
      *
  Analysing:
    The 10.96.0.1 is the kubernetes cluster ip, it's not a real ip, it depends iptables on local host to translate to ip of the master.
    The iptables rules is generated by kube-proxy, since no below iptable rule exist, we need check kube-proxy

Chain KUBE-SEP-WJ45QRM6NUHACNIV (1 references)
target     prot opt source               destination         
KUBE-MARK-MASQ  all  --  kubernetes-master    anywhere            
DNAT       tcp  --  anywhere             anywhere             tcp to:192.168.22.52:6443

   2. kube-proxy miss some kernel modules
   Check log of pod kube-proxy, 
W0819 02:35:37.917090       1 proxier.go:513] Failed to load kernel module ip_vs with modprobe. You can ignore this message when kube-proxy is running inside container without mounting /lib/modules
W0819 02:35:37.924262       1 proxier.go:513] Failed to load kernel module ip_vs_rr with modprobe. You can ignore this message when kube-proxy is running inside container without mounting /lib/modules
W0819 02:35:37.933260       1 proxier.go:513] Failed to load kernel module ip_vs_wrr with modprobe. You can ignore this message when kube-proxy is running inside container without mounting /lib/modules
W0819 02:35:37.938814       1 proxier.go:513] Failed to load kernel module ip_vs_sh with modprobe. You can ignore this message when kube-proxy is running inside container without mounting /lib/modules 
   Analysing:
   Rebuild the kernel with those feature enabled

   3. kube-proxy failed to run iptables-restore
   Anaysing:
   Try to run kube-proxy in verbose mode, 
   `kubectl -n kube-system edit daemonset kube-proxy`
   print out iptables rules
   run iptable-restore on these rules, and we find 
`-A KUBE-POSTROUTING -m comment --comment "kubernetes service traffic requiring SNAT" -m mark --mark 0x00004000/0x00004000 -j MASQUERADE`
   is the root cause iptables-restore failed
   it's also due to missing of some features of iptables, just enable it



# Issues on Network
   1. the nework architecture

   2. ping between pods
sudo iptables -P FORWARD ACCEPT

   3. ping the router 192.168.22.1
add route to pod in the router's table

   4. ping 8.8.8.8
add NAT in the router 

   5. ping internet
set the dns




You’ll find this post in your `_posts` directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run `jekyll serve`, which launches a web server and auto-regenerates your site when a file is updated.

To add new posts, simply add a file in the `_posts` directory that follows the convention `YYYY-MM-DD-name-of-post.ext` and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.

Jekyll also offers powerful support for code snippets:



Check out the [Jekyll docs][jekyll-docs] for more info on how to get the most out of Jekyll. File all bugs/feature requests at [Jekyll’s GitHub repo][jekyll-gh]. If you have questions, you can ask them on [Jekyll Talk][jekyll-talk].

[jekyll-docs]: https://jekyllrb.com/docs/home
[jekyll-gh]:   https://github.com/jekyll/jekyll
[jekyll-talk]: https://talk.jekyllrb.com/
