---
layout: post
title:  "Setup NXP LS1043A as a Kubernetes Node in Flannel networking"
categories: jekyll update
---

This post tell how to setup NXP LS1043A as a Kubernetes node in flannel networking, and talk about some issues during the process.

# Overview

This process need two PCs or VMs, one as Kubernetes master and one as node. Also, one NXP LS1043 box is needed to act as another kubernetes node.

![overview1](/images/nxp1043ask8snode-model.png)

This setup at last deploys a kubernetes service, with two kubernetes pods, each has one container.

![architecture](/images/nxp1043ask8snode-architecture.png)

Flannel is a simple and easy way to configure a layer 3 network fabric designed for Kubernetes.
Here flannel is employed to provide connection between pods.

# Play with PCs
[This reference](https://linuxconfig.org/how-to-install-kubernetes-on-ubuntu-18-04-bionic-beaver-linux) is a guide to setup kubernetes on standard PCs.   
Before continue, please setup a master and a node on two standard PCs according to this reference.

# Setup NXP 1043A

```
dpkg -r edgescale-agent
apt-get install docker-ce-cli
apt-get install kubeadm kubectl kubelet
```

# Bring all together

  * on master

```bash
sudo kubeadm reset
sudo kubeadm init --pod-network-cidr=10.217.0.0/16 -v8
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

kubectl get pods -o wide --all-namespaces
#wait till ready of all pods

kubectl create -f kube-flannel.yml

kubectl get pods -o wide --all-namespaces
#wait till ready of all pods

kubectl apply -f service1.yaml
```

[service1.yaml](https://raw.githubusercontent.com/vewe-richard/vewe-richard.github.io/master/data/service1.yaml)

[kube-flannel.yml](https://raw.githubusercontent.com/vewe-richard/vewe-richard.github.io/master/data/kube-flannel.yml)

  * on node2 (the PC node)

```bash
sudo kubeadm reset
sudo kubeadm join 192.168.22.52:6443 --token krhd8f.bsv1h9761uh0lhqf \
    --discovery-token-ca-cert-hash sha256:7992b9fa9788e0b54c671cab9839c4596bd575b616dd2d23010f85485a21d2f9
```
Please replace token and hash strings with your own.

  * on node1 (the NXP 1043A)

```bash
sudo kubeadm reset
sudo kubeadm join 192.168.22.52:6443 --token krhd8f.bsv1h9761uh0lhqf \
    --discovery-token-ca-cert-hash sha256:7992b9fa9788e0b54c671cab9839c4596bd575b616dd2d23010f85485a21d2f9
```
Please replace token and hash strings with your own.

# Issues and how to fix
   This post mainly focus on discussion of some issues during the process, hope it help someone encounter similiar issues.

#### 1. dial tcp 10.96.0.1:443: i/o timeout

  Log of pod `kube-system/kube-flannel-ds-arm64-jg5g6`, 
```
I0819 02:38:10.208759      22 main.go:514] Determining IP address of default interface
I0819 02:38:10.213218      22 main.go:527] Using interface with name fm1-mac3 and address 192.168.22.60
I0819 02:38:10.213314      22 main.go:544] Defaulting external address to interface address (192.168.22.60)
E0819 02:38:40.223479      22 main.go:241] Failed to create SubnetManager: error retrieving pod spec for 'kube-system/kube-flannel-ds-arm64-jg5g6': Get https://10.96.0.1:443/api/v1/namespaces/kube-system/pods/kube-flannel-ds-arm64-jg5g6: dial tcp 10.96.0.1:443: i/o timeout
```
   Analysing:

10.96.0.1 is the kubernetes cluster ip, it's not a real ip, it is a virtual ip of master to provide apiserver service. It depends on iptables on local host(node2) to translate to ip of the master.
The iptables rules are generated by kube-proxy, since no below iptable rule exist, we need check kube-proxy
```
Chain KUBE-SEP-WJ45QRM6NUHACNIV (1 references)
target     prot opt source               destination         
KUBE-MARK-MASQ  all  --  kubernetes-master    anywhere            
DNAT       tcp  --  anywhere             anywhere             tcp to:192.168.22.52:6443
```

#### 2. kube-proxy start failed

   Log of pod kube-proxy, 

```
W0819 02:35:37.917090       1 proxier.go:513] Failed to load kernel module ip_vs with modprobe. You can ignore this message when kube-proxy is running inside container without mounting /lib/modules
W0819 02:35:37.924262       1 proxier.go:513] Failed to load kernel module ip_vs_rr with modprobe. You can ignore this message when kube-proxy is running inside container without mounting /lib/modules
W0819 02:35:37.933260       1 proxier.go:513] Failed to load kernel module ip_vs_wrr with modprobe. You can ignore this message when kube-proxy is running inside container without mounting /lib/modules
W0819 02:35:37.938814       1 proxier.go:513] Failed to load kernel module ip_vs_sh with modprobe. You can ignore this message when kube-proxy is running inside container without mounting /lib/modules 
```

   Analysing:

Rebuild the kernel with those features enabled

#### 3. kube-proxy failed to run iptables-restore

   Anaysing:

Try to run kube-proxy in verbose mode to print out iptables rules.

```
#open kube-proxy daemonset, add -v in the parameter list
kubectl -n kube-system edit daemonset kube-proxy
```

Run iptable-restore on these rules, and we find below rule cause failure of iptables-restore.
```
-A KUBE-POSTROUTING -m comment --comment "kubernetes service traffic requiring SNAT" -m mark --mark 0x00004000/0x00004000 -j MASQUERADE
```

It's also due to missing of some features of iptables, enable all netfilter features to avoid such errors.


# Issues on Network
* the nework architecture

* can not ping between pods

Try below on both nodes.
```
sudo iptables -P FORWARD ACCEPT
```

* can not ping the router 192.168.22.1

Add route to pod in the router's routing table

* can not ping 8.8.8.8

Add NAT in the router 

* can not ping internet

Set the dns, in /etc/resolv.conf




